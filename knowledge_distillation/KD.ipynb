{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIcekQe4y7pS",
        "outputId": "3678f8a8-229e-4480-8a1f-6ca1d89d1a31"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9eXFUjUy7sC"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Vs-2GQGozQ0f",
        "outputId": "e9914fa1-be3d-4f6d-f178-adef3c518069"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "shutil.copy(\"/content/drive/MyDrive/DL/Dataset_train.zip\",\"/content/Dataset_train.zip\")\n",
        "shutil.copy(\"/content/drive/MyDrive/DL/Dataset_test.zip\",\"/content/Dataset_test.zip\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxdZew-g8J09"
      },
      "outputs": [],
      "source": [
        "zip_path = \"/content/Dataset_train.zip\"\n",
        "with zipfile.ZipFile(zip_path,\"r\") as zip_ref:\n",
        "  zip_ref.extractall(\"./Dataset/\")\n",
        "\n",
        "import zipfile\n",
        "zip_path = \"/content/Dataset_test.zip\"\n",
        "with zipfile.ZipFile(zip_path,\"r\") as zip_ref:\n",
        "  zip_ref.extractall(\"./Dataset_test/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwHrgWua2aHU",
        "outputId": "5526aab1-4b01-4fa7-ede7-a9574c3c9145"
      },
      "outputs": [],
      "source": [
        "image_size = (1024, 1024)\n",
        "batch_size = 64\n",
        "IMG_SIZE =256\n",
        "\n",
        "\n",
        "resize_and_rescale = tf.keras.Sequential([\n",
        "  layers.Resizing(IMG_SIZE, IMG_SIZE),\n",
        "  layers.Rescaling(1./255)\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
        "resizing_layer = tf.keras.layers.Resizing(IMG_SIZE, IMG_SIZE)\n",
        "flip_layer = tf.keras.layers.RandomRotation(0.1)\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"/content/Dataset/\",\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=1337,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size,\n",
        ")\n",
        "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "train_ds = train_ds.map(lambda x, y: (resizing_layer(x), y))\n",
        "train_ds = train_ds.map(lambda x, y: (flip_layer(x), y))\n",
        "\n",
        "\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"/content/Dataset/\",\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=1337,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size,\n",
        ")\n",
        "\n",
        "val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "val_ds = val_ds.map(lambda x, y: (resizing_layer(x), y))\n",
        "\n",
        "\n",
        "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"/content/Dataset_test/\",\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size,\n",
        ")\n",
        "\n",
        "\n",
        "test_ds = test_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "test_ds = test_ds.map(lambda x, y: (resizing_layer(x), y))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2i_C420r1bRc"
      },
      "outputs": [],
      "source": [
        "# import keras.backend as K\n",
        "# def get_f1(y_true, y_pred): #taken from old keras source code\n",
        "#     true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "#     possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "#     predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "#     precision = true_positives / (predicted_positives + K.epsilon())\n",
        "#     recall = true_positives / (possible_positives + K.epsilon())\n",
        "#     f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "#     return f1_val\n",
        "\n",
        "\n",
        "teacher_model = keras.models.load_model(\n",
        "    \"/content/drive/MyDrive/DL/xception_final/save_at_20.h5\"\n",
        ")\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rws62nQy_wt"
      },
      "outputs": [],
      "source": [
        "class Distiller(keras.Model):\n",
        "    def __init__(self, student, teacher):\n",
        "        super(Distiller, self).__init__()\n",
        "        self.teacher = teacher\n",
        "        self.student = student\n",
        "\n",
        "\n",
        "    def compile(self,optimizer,metrics,student_loss_fn,distillation_loss_fn,alpha=0.1,temperature=3,):\n",
        "\n",
        "        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\n",
        "        self.student_loss_fn = student_loss_fn\n",
        "        self.distillation_loss_fn = distillation_loss_fn\n",
        "        self.alpha = alpha\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def train_step(self, data):\n",
        "        x, y = data\n",
        "\n",
        "        teacher_predictions = self.teacher(x, training=False)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            student_predictions = self.student(x, training=True)\n",
        "            student_loss = self.student_loss_fn(y, student_predictions)\n",
        "            distillation_loss = self.distillation_loss_fn(\n",
        "                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
        "                tf.nn.softmax(student_predictions / self.temperature, axis=1),\n",
        "            )\n",
        "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
        "\n",
        "        trainable_vars = self.student.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "        self.compiled_metrics.update_state(y, student_predictions)\n",
        "\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update(\n",
        "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n",
        "        )\n",
        "        return results\n",
        "\n",
        "    def test_step(self, data):\n",
        "        x, y = data\n",
        "        y_prediction = self.student(x, training=False)\n",
        "        student_loss = self.student_loss_fn(y, y_prediction)\n",
        "        self.compiled_metrics.update_state(y, y_prediction)\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update({\"student_loss\": student_loss})\n",
        "        return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cGZAUi1y_y_"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def make_model(input_shape, num_classes):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    # x = layers.Conv2D(64, 3, padding=\"same\")(x)\n",
        "    # x = layers.BatchNormalization()(x)\n",
        "    # x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    # x = layers.Conv2D(128, 3, padding=\"same\")(x)\n",
        "    # x = layers.BatchNormalization()(x)\n",
        "    # x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    # x = layers.Conv2D(256, 3, padding=\"same\")(x)\n",
        "    # x = layers.BatchNormalization()(x)\n",
        "    # x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "\n",
        "    previous_block_activation = x  # Set aside residual\n",
        "    #for size in [128]:\n",
        "    for size in [128,256]:\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
        "\n",
        "       \n",
        "        residual = layers.Conv2D(size, 1, strides=2, padding=\"same\")(\n",
        "            previous_block_activation\n",
        "        )\n",
        "        x = layers.add([x, residual])  \n",
        "        previous_block_activation = x  \n",
        "\n",
        "    x = layers.SeparableConv2D(1024, 3, padding=\"same\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "    activation = \"softmax\"\n",
        "    units = num_classes\n",
        "\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    outputs = layers.Dense(units, activation=activation)(x)\n",
        "    return keras.Model(inputs, outputs)\n",
        "\n",
        "\n",
        "student_model = make_model(input_shape=(256,256) + (3,), num_classes=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYY6G6Ab9Arv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bL4N1s9zMTD",
        "outputId": "c0460abc-b7bb-4287-a34e-134d6ee01ee8"
      },
      "outputs": [],
      "source": [
        "distiller = Distiller(student=student_model, teacher=teacher_model)\n",
        "distiller.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        "    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    distillation_loss_fn=keras.losses.KLDivergence(),\n",
        "    alpha=0.1,\n",
        "    temperature=5,\n",
        ")\n",
        "\n",
        "distiller.fit(train_ds, epochs=7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8DT5bzfXjs9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "predictions = np.array([])\n",
        "labels =  np.array([])\n",
        "for x, y in test_ds:\n",
        "  temp = student_model.predict(x)\n",
        "  predictions = np.concatenate([predictions, student_model.predict(x).argmax(axis =1)+1])\n",
        "  labels = np.concatenate([labels, y.numpy()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWAfbjJSsWk1",
        "outputId": "0220105f-4bfc-47b7-ee3b-27e8b255e442"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(labels,predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUr--Av75RDm",
        "outputId": "62266524-474e-4292-d68c-49afb63692fb"
      },
      "outputs": [],
      "source": [
        "keras_file = \"/content/drive/MyDrive/DL/KD/128_256__5_7_cnn_student.h5\"\n",
        "student_model.save(keras_file)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIPulGUtDCh-"
      },
      "outputs": [],
      "source": [
        "! pip install -q tensorflow-model-optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sl9cYLbX2Kr",
        "outputId": "9088ce05-91c1-4b40-f99f-86ff916d6c42"
      },
      "outputs": [],
      "source": [
        "student_model.save(\"/content/drive/MyDrive/DL/KD/128_256__5_7_cnn_student.h5\")\n",
        "pruned_keras_file = \"/content/drive/MyDrive/DL/KD/128_256__5_7_cnn_student_strip.h5\"\n",
        "pruned_tflite_file = \"/content/drive/MyDrive/DL/KD/128_256__5_7_cnn_student_strip_tflite.h5\"\n",
        "import tempfile\n",
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
        "\n",
        "batch_size = 32\n",
        "epochs = 2\n",
        "\n",
        "end_step = 300*epochs\n",
        "pruning_params = {\n",
        "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
        "                                                               final_sparsity=0.80,\n",
        "                                                               begin_step=0,\n",
        "                                                               end_step=end_step)\n",
        "}\n",
        "\n",
        "model_for_pruning = prune_low_magnitude(student_model, **pruning_params)\n",
        "\n",
        "model_for_pruning.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "logdir = tempfile.mkdtemp()\n",
        "\n",
        "callbacks = [\n",
        "  tfmot.sparsity.keras.UpdatePruningStep(),\n",
        "  tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
        "]\n",
        "\n",
        "model_for_pruning.fit(train_ds, epochs=epochs, callbacks=callbacks, validation_data=val_ds,\n",
        ")\n",
        "\n",
        "predictions = np.array([])\n",
        "labels =  np.array([])\n",
        "\n",
        "for x, y in test_ds:\n",
        "  predictions = np.concatenate([predictions, model_for_pruning.predict(x).argmax(axis =1)+1])\n",
        "  labels = np.concatenate([labels, y.numpy()])\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(labels,predictions))\n",
        "\n",
        "\n",
        "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
        "\n",
        "tf.keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\n",
        "print('Saved pruned Keras model to:', pruned_keras_file)\n",
        "\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
        "pruned_tflite_model = converter.convert()\n",
        "\n",
        "\n",
        "with open(pruned_tflite_file, 'wb') as f:\n",
        "  f.write(pruned_tflite_model)\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "def get_gzipped_model_size(file):\n",
        "\n",
        "\n",
        "  _, zipped_file = tempfile.mkstemp('.zip')\n",
        "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
        "    f.write(file)\n",
        "\n",
        "  return os.path.getsize(zipped_file)\n",
        "print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\n",
        "print(\"Size of gzipped pruned Keras model: %.2f bytes\" % (get_gzipped_model_size(pruned_keras_file)))\n",
        "print(\"Size of gzipped pruned TFlite model: %.2f bytes\" % (get_gzipped_model_size(pruned_tflite_file)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSNRsqwJTpLn"
      },
      "outputs": [],
      "source": [
        "del student_model, teacher_model, model_for_pruning, model_for_export"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8Xxp7P9mhUJ"
      },
      "source": [
        "# References\n",
        "https://keras.io/examples/vision/image_classification_from_scratch/\n",
        "<br>\n",
        "https://keras.io/examples/vision/knowledge_distillation/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gM5WeTkmvl0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "KD.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
